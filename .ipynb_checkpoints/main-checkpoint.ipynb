{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import generator\n",
    "import discriminator\n",
    "import music_helpers\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = False\n",
    "# VOCAB_SIZE = 5000\n",
    "MAX_SEQ_LEN = 20\n",
    "START_LETTER = 0\n",
    "BATCH_SIZE = 32\n",
    "MLE_TRAIN_EPOCHS = 100\n",
    "ADV_TRAIN_EPOCHS = 50\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "GEN_EMBEDDING_DIM = 32\n",
    "GEN_HIDDEN_DIM = 32\n",
    "DIS_EMBEDDING_DIM = 64\n",
    "DIS_HIDDEN_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 1\n",
    "\n",
    "oracle_samples is a tensor of 10,000 sequences each 20 integers long\n",
    "where each integer represents a token in a vocabulary size of 5000\n",
    "\n",
    "1. replace oracle_samples with mozart_text with size (10000,20) in 10,000 sequences of 20 integers long, each an intger\n",
    "    - add function to helper file\n",
    "2. change the following hyperparameters:\n",
    "    - BATCH_SIZE\n",
    "    - MAX_SEQ_LEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load True Distribution Generator\n",
    "# Load True Distribution Data Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_music_file(file):\n",
    "    with open(file, 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # get vocabulary set\n",
    "    words = sorted(tuple(set(text.split())))\n",
    "    n = len(words)\n",
    "\n",
    "    # create word-integer encoder/decoder\n",
    "    word2int = dict(zip(words, list(range(n))))\n",
    "    int2word = dict(zip(list(range(n)), words))\n",
    "\n",
    "    # encode all words in dataset into integers\n",
    "    encoded = np.array([word2int[word] for word in text.split()])\n",
    "    \n",
    "    return n, word2int, int2word, encoded\n",
    "\n",
    "def batch_music_samples(data, seq_len):\n",
    "    batches = []\n",
    "    i=0\n",
    "    while i+seq_len <= len(data):\n",
    "        batch = list(data[i:i+seq_len])\n",
    "        batches.append(batch)\n",
    "        i += seq_len\n",
    "    return batches\n",
    "    \n",
    "def train_val_split(data, train_size):\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    train = torch.tensor(data[:train_size]).type(torch.LongTensor)\n",
    "    val = torch.tensor(data[train_size:]).type(torch.LongTensor)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozart_data = \"./data/mozart.txt\"\n",
    "\n",
    "VOCAB_SIZE, word2int, int2word, encoded_data = load_music_file(mozart_data)\n",
    "\n",
    "# returns list of (567850,20)\n",
    "real_data_samples = batch_music_samples(encoded_data, MAX_SEQ_LEN)\n",
    "\n",
    "POS_NEG_SAMPLES = 10000\n",
    "\n",
    "# returns tensor of (500000, 20) and (67850, 20)\n",
    "real_train, real_val = train_val_split(real_data_samples, POS_NEG_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 1 complete\n",
    "\n",
    "At this point:\n",
    "1. the mozart data has been ingested\n",
    "2. it has been prepared for train (500000, 20) and val (67850, 20)\n",
    "3. hyperparameters have been modified to accept a larger dataset and smaller vocab size\n",
    "\n",
    "\n",
    "May have to change the starting letter?\n",
    "\n",
    "Why do I need to know oracle loss? To compare the difference between the oracle and the generator?\n",
    "\n",
    "How fast does it train on the GPU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Fake Generator\n",
    "# Build Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator.Generator(GEN_EMBEDDING_DIM, GEN_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)\n",
    "dis = discriminator.Discriminator(DIS_EMBEDDING_DIM, DIS_HIDDEN_DIM, VOCAB_SIZE, MAX_SEQ_LEN, gpu=CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cast tensors to GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    gen = gen.cuda()\n",
    "    dis = dis.cuda()\n",
    "    real_train = real_train.cuda()\n",
    "    real_val = real_val.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 2\n",
    "\n",
    "1. Remove the calculation of Oracle Loss\n",
    "    - Altnernatively, change how the oracle samples or generates data for batch NLLLoss calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train Generator MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_MLE(gen, gen_opt, real_data_samples, epochs):\n",
    "    \"\"\"\n",
    "    Max Likelihood Pretraining for the generator\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # just an log for Generator MLE training\n",
    "        \n",
    "        print('epoch %d : ' % (epoch + 1), end='')\n",
    "        sys.stdout.flush()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # real_data_samples is oracles_samples\n",
    "        # POS_NEG_SAMPLES = sample size = 10,000\n",
    "        # BATCH_SIZE = 32\n",
    "        # so i will be [0, 32, 64, 96, 128, ...]\n",
    "\n",
    "        for i in range(0, POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "            \n",
    "            \"\"\"\n",
    "            each input and target is size (32, 20) = (BATCH_SIZE, MAX_SEQ_LEN)\n",
    "            \n",
    "            \n",
    "            FOR EXAMPLE:\n",
    "            \n",
    "            -INPUT-\n",
    "            \n",
    "            start_letter = 0\n",
    "            \n",
    "            samples = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
    "            \n",
    "            \n",
    "            -OUTPUTS-\n",
    "            \n",
    "            input = [[0, 11, 12, 13],\n",
    "                     [0, 15, 16, 17],\n",
    "                     [0, 19, 20, 21]]\n",
    "            \n",
    "            target = [[11, 12, 13, 14],\n",
    "                      [15, 16, 17, 18],\n",
    "                      [19, 20, 21, 22]]\n",
    "                      \n",
    "            REAL EXAMPLE:\n",
    "            \n",
    "            start_letter: 0\n",
    "            input: tensor([   0, 4766,  468, 2145,  938, 2625,   23, 1038, 2449, 2065, 3364,  429,\n",
    "                           2323,  784, 2985, 2985,  203, 2912, 2707, 1370])\n",
    "            target: tensor([4766,  468, 2145,  938, 2625,   23, 1038, 2449, 2065, 3364,  429, 2323,\n",
    "                            784, 2985, 2985,  203, 2912, 2707, 1370, 2515])\n",
    "            \n",
    "            \"\"\"\n",
    "            \n",
    "            inp, target = music_helpers.prepare_generator_batch(real_data_samples[i:i + BATCH_SIZE], start_letter=START_LETTER,\n",
    "                                                          gpu=CUDA)\n",
    "\n",
    "            \"\"\"\n",
    "            in order:\n",
    "            \n",
    "            1. set gradients to be zero\n",
    "            2. compute training loss\n",
    "            3. backpropagate gradient\n",
    "            4. update weights of nn\n",
    "            \"\"\"\n",
    "            \n",
    "            gen_opt.zero_grad()\n",
    "            loss = gen.batchNLLLoss(inp, target)\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_loss += loss.data.item()\n",
    "\n",
    "            if (i / BATCH_SIZE) % ceil(\n",
    "                            ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                print('.', end='')\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        # each loss in a batch is loss per sample\n",
    "        total_loss = total_loss / ceil(POS_NEG_SAMPLES / float(BATCH_SIZE)) / MAX_SEQ_LEN\n",
    "\n",
    "        print(' average_train_NLL = %.4f' % (total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification 3\n",
    "1. Change how positive validation (real data) is sampled\n",
    "    - create a function to choose from the positive validation\n",
    "    \n",
    "2. Understand how def batchwise_oracle_nll works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_sample(data, n):\n",
    "    torch.tensor(data)\n",
    "    sample_idx = random.sample(range(1, len(data)), n)\n",
    "    return data[sample_idx]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generator MLE Training...\n",
      "epoch 1 : .......... average_train_NLL = 3.6767\n"
     ]
    }
   ],
   "source": [
    "# GENERATOR MLE TRAINING\n",
    "print('Starting Generator MLE Training...')\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=1e-2)\n",
    "train_generator_MLE(gen, gen_optimizer, real_train, MLE_TRAIN_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, dis_opt, real_data_samples, generator, real_val, d_steps, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    PREPARE VALIDATION SET BEFORE TRAINING\n",
    "    \n",
    "    pos_val: tensor of (100, 20) = (BATCH_SIZE, SEQ_LEN) \n",
    "    data sample generated by the TRUE distribution\n",
    "    \n",
    "    neg_val: tensor of (100, 20) = (BATCH_SIZE, SEQ_LEN) \n",
    "    data sample generated by the FAKE distribution\n",
    "    \n",
    "    val_input: tensor of (200, 20)\n",
    "    data sample of pos_val and neg_val joined\n",
    "    \n",
    "    val_target: tensor of (200)\n",
    "    binary label for val_input indicating which is TRUE and which is FAKE\n",
    "    \n",
    "    EXAMPLE:\n",
    "    \n",
    "    pos_val: tensor([3919, 3055,  295,  221, 3468,  973, ...])\n",
    "    neg_val: tensor([4542, 2385, 2421, 4289,  135, 4437, ...])\n",
    "    val_inp: tensor([3784, 4782, 2792, 4431, 3654, 2415, ...])\n",
    "    val_target: tensor([0., 1., 1., 0., 1., 0., 1., 0., 0., 0., ...])\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # generating a small validation set before training (using oracle and generator)\n",
    "#     pos_val = oracle.sample(100)\n",
    "    pos_val = positive_sample(real_val, 100)\n",
    "    neg_val = generator.sample(100)\n",
    "    val_inp, val_target = music_helpers.prepare_discriminator_data(pos_val, neg_val, gpu=CUDA)\n",
    "\n",
    "    for d_step in range(d_steps):\n",
    "        \n",
    "        \"\"\"\n",
    "        generate 10,000 FAKE data samples of length 20 sequences\n",
    "        s: tensor of (10000, 20)\n",
    "        \n",
    "        concatenate with TRUE data samples from oracle\n",
    "        real_data_samples: tensor of (10000, 20)\n",
    "        \n",
    "        then shuffle with the binary labels to get:\n",
    "        dis_inp: tensor of (20000, 20)\n",
    "        dis_target: tensor of (20000) containing 1 or 0 for TRUE of FAKE data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        s = music_helpers.batchwise_sample(generator, POS_NEG_SAMPLES, BATCH_SIZE)\n",
    "        \n",
    "        dis_inp, dis_target = music_helpers.prepare_discriminator_data(real_data_samples, s, gpu=CUDA)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            \n",
    "            # loop through all 20,0000 data samples\n",
    "\n",
    "            for i in range(0, 2 * POS_NEG_SAMPLES, BATCH_SIZE):\n",
    "                \n",
    "                # subset input based on batch size\n",
    "                inp, target = dis_inp[i:i + BATCH_SIZE], dis_target[i:i + BATCH_SIZE]\n",
    "                \n",
    "                # zero all gradients\n",
    "                dis_opt.zero_grad()\n",
    "                \n",
    "                # get binary output from discriminator based on input\n",
    "                out = discriminator.batchClassify(inp)\n",
    "                \n",
    "                # re-initialize loss function\n",
    "                loss_fn = nn.BCELoss()\n",
    "                \n",
    "                # calculate loss based on output and target\n",
    "                loss = loss_fn(out, target)\n",
    "                \n",
    "                # backpropagate\n",
    "                loss.backward()\n",
    "                \n",
    "                # update weights\n",
    "                dis_opt.step()\n",
    "\n",
    "                total_loss += loss.data.item()\n",
    "                total_acc += torch.sum((out>0.5)==(target>0.5)).data.item()\n",
    "\n",
    "                if (i / BATCH_SIZE) % ceil(ceil(2 * POS_NEG_SAMPLES / float(\n",
    "                        BATCH_SIZE)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            total_loss /= ceil(2 * POS_NEG_SAMPLES / float(BATCH_SIZE))\n",
    "            total_acc /= float(2 * POS_NEG_SAMPLES)\n",
    "\n",
    "            val_pred = discriminator.batchClassify(val_inp)\n",
    "            print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
    "                total_loss, total_acc, torch.sum((val_pred>0.5)==(val_target>0.5)).data.item()/200.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Discriminator Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-step 1 epoch 1 : .......... average_loss = 0.4647, train_acc = 0.7670, val_acc = 0.9200\n"
     ]
    }
   ],
   "source": [
    "# PRETRAIN DISCRIMINATOR\n",
    "print('\\nStarting Discriminator Training...')\n",
    "dis_optimizer = optim.Adagrad(dis.parameters())\n",
    "train_discriminator(dis, dis_optimizer, real_train, gen, real_val, 50, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_PG(gen, gen_opt, dis, num_batches):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for num_batches batches.\n",
    "    \"\"\"\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        \n",
    "        \"\"\"\n",
    "        1. get generator to generate samples = s (a batch of 64 sequences of length 20)\n",
    "        2. prepare input (pre-pended target) and target based on generated samples(same as train_generator_MLE)\n",
    "        3. get discriminator to return a sigmoid value between 0 and 1 on whether generated samples are REAL or FAKE\n",
    "        4. reward is a tensor of size (64) containing the DIS sigmoid value of every sequence in the batch\n",
    "        5. the sigmoid values in reward is used as a multiplier in the loss function\n",
    "            a. if value is near 0 (DIS classified as FAKE) then loss is small (not so negative)\n",
    "            b. if value is near 1 (DIS classified as REAL) then loss is large (very negative)\n",
    "        \"\"\"\n",
    "        \n",
    "        s = gen.sample(BATCH_SIZE*2)        # 64 works best\n",
    "        inp, target = music_helpers.prepare_generator_batch(s, start_letter=START_LETTER, gpu=CUDA)\n",
    "        rewards = dis.batchClassify(target)\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        pg_loss = gen.batchPGLoss(inp, target, rewards)\n",
    "        pg_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "    # sample from generator and compute oracle NLL\n",
    "#     oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "#                                                    start_letter=START_LETTER, gpu=CUDA)\n",
    "\n",
    "#     print(' oracle_sample_NLL = %.4f' % oracle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Adversarial Training...\n",
      "\n",
      "--------\n",
      "EPOCH 1\n",
      "--------\n",
      "\n",
      "Adversarial Training Generator: \n",
      "Adversarial Training Discriminator : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d-step 1 epoch 1 : .......... average_loss = 0.1828, train_acc = 0.9277, val_acc = 0.9100\n"
     ]
    }
   ],
   "source": [
    "# ADVERSARIAL TRAINING\n",
    "print('\\nStarting Adversarial Training...')\n",
    "# oracle_loss = helpers.batchwise_oracle_nll(gen, oracle, POS_NEG_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN,\n",
    "#                                            start_letter=START_LETTER, gpu=CUDA)\n",
    "# print('\\nInitial Oracle Sample Loss : %.4f' % oracle_loss)\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "for epoch in range(ADV_TRAIN_EPOCHS):\n",
    "    print('\\n--------\\nEPOCH %d\\n--------' % (epoch+1))\n",
    "    # TRAIN GENERATOR\n",
    "    print('\\nAdversarial Training Generator: ', end='')\n",
    "    sys.stdout.flush()\n",
    "    train_generator_PG(gen, gen_optimizer, dis, 1)\n",
    "\n",
    "    # TRAIN DISCRIMINATOR\n",
    "    print('\\nAdversarial Training Discriminator : ')\n",
    "    train_discriminator(dis, dis_optimizer, real_train, gen, real_val, 5, 3)\n",
    "    \n",
    "    # generate output samples\n",
    "    samples20 = gen.sample(n_samples)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample20 = ' '.join([int2word[x] for x in list(np.array(samples20[i]))])        \n",
    "        with open(\"./data/output/result_epoch\" + str(epoch) + \"_sample\" + str(i) + \"_20.txt\", \"w\") as outfile:\n",
    "                outfile.write(sample20)\n",
    "                \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
